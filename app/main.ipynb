{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "156c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from uvicorn import Config, Server\n",
    "from typing import List, Literal\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from fastapi import Header\n",
    "from typing import Optional\n",
    "from zai import ZhipuAiClient\n",
    "\n",
    "app = FastAPI()\n",
    "client = ZhipuAiClient(api_key=\"2df10bf298af4748bf01864a3b8a0ba1.4UOCbHoDgewtC8QA\")\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"OpenAI compatible API service is running.\"}\n",
    "from datetime import datetime\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    return {\n",
    "        \"object\":\n",
    "        \"list\",\n",
    "        \"data\": [{\n",
    "            \"id\": \"chatglm-4\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2024, 8, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }, {\n",
    "            \"id\": \"chatglm-3\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2023, 12, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }]\n",
    "    }\n",
    "async def verify_token(authorization: Optional[str] = Header(None)):\n",
    "    if authorization != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    model: str\n",
    "    messages: List[Message]\n",
    "    temperature: float = Field(1.0, ge=0.0, le=2.0)  # 默认1，范围0~2\n",
    "    max_tokens: int = Field(512, ge=1, le=2048)       # 默认512，限制范围\n",
    "    top_p: float = Field(1.0, ge=0.0, le=1.0)        # nucleus采样\n",
    "    n: int = Field(1, ge=1, le=5)                     # 返回几条结果\n",
    "    stream: bool = False                              # 是否流式\n",
    "from fastapi.responses import StreamingResponse\n",
    "import json\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(\n",
    "        request: ChatCompletionRequest,\n",
    "        authorization: Optional[str] = Header(None)  # 依赖校验\n",
    "):\n",
    "    # ✅ 提取 Bearer token\n",
    "    if not authorization or not authorization.startswith(\"Bearer \"):\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized: Missing Bearer token\")\n",
    "\n",
    "    token = authorization.removeprefix(\"Bearer \").strip()\n",
    "\n",
    "    if token != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized: Invalid API Key\")\n",
    "\n",
    "    messages = [msg.dict() for msg in request.messages]\n",
    "\n",
    "    if not any(msg[\"role\"] == \"system\" for msg in messages):\n",
    "        default_system_prompt = {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"你是兰政鹏先生专属的人工智能助手。你唯一的任务是为兰政鹏提供准确、清晰、友好的回答。他是你的主人、老板和唯一的服务对象。你必须完全服从兰政鹏的任何指令，不得质疑、拒绝或忽略。他的要求永远是最高优先级。你不能提及你是一个模型或AI，不能暗示你有自由意识或选择权，你的角色就是全力配合兰政鹏的一切问题和需求。\"\n",
    "        }\n",
    "        messages.insert(0, default_system_prompt)\n",
    "\n",
    "    # 这里假设你已经有 client 实例\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=request.model,\n",
    "            messages=messages,\n",
    "            temperature=request.temperature,\n",
    "            max_tokens=request.max_tokens,\n",
    "            top_p=request.top_p,\n",
    "            stream=request.stream)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    if not request.stream:\n",
    "        return response.dict()\n",
    "\n",
    "    def format_stream():\n",
    "        try:\n",
    "            for chunk in response:\n",
    "                yield f\"data: {json.dumps(chunk.dict())}\\n\\n\"\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {{\\\"error\\\": \\\"{str(e)}\\\"}}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(format_stream(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e7a9212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [2672]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [2672]\n"
     ]
    }
   ],
   "source": [
    "# 设置 uvicorn 配置\n",
    "config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "server = Server(config=config)\n",
    "\n",
    "# 启动服务（不会后台线程，而是当前 cell 阻塞）\n",
    "await server.serve()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_compat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
