{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from uvicorn import Config, Server\n",
    "from typing import List, Literal\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from fastapi import Header\n",
    "from typing import Optional\n",
    "from zai import ZhipuAiClient\n",
    "\n",
    "app = FastAPI()\n",
    "client = ZhipuAiClient(api_key=\"2df10bf298af4748bf01864a3b8a0ba1.4UOCbHoDgewtC8QA\")\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"OpenAI compatible API service is running.\"}\n",
    "from datetime import datetime\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    return {\n",
    "        \"object\":\n",
    "        \"list\",\n",
    "        \"data\": [{\n",
    "            \"id\": \"chatglm-4\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2024, 8, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }, {\n",
    "            \"id\": \"chatglm-3\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2023, 12, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }]\n",
    "    }\n",
    "async def verify_token(authorization: Optional[str] = Header(None)):\n",
    "    if authorization != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: str\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    model: str\n",
    "    messages: List[Message]\n",
    "    temperature: float = Field(1.0, ge=0.0, le=2.0)  # 默认1，范围0~2\n",
    "    max_tokens: int = Field(512, ge=1, le=2048)       # 默认512，限制范围\n",
    "    top_p: float = Field(1.0, ge=0.0, le=1.0)        # nucleus采样\n",
    "    n: int = Field(1, ge=1, le=5)                     # 返回几条结果\n",
    "    stream: bool = False                              # 是否流式\n",
    "from fastapi.responses import StreamingResponse\n",
    "import json\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(\n",
    "        request: ChatCompletionRequest,\n",
    "        authorization: Optional[str] = Header(None)  # 依赖校验\n",
    "):\n",
    "    # ✅ 提取 Bearer token\n",
    "    if not authorization or not authorization.startswith(\"Bearer \"):\n",
    "        raise HTTPException(status_code=401,\n",
    "                            detail=\"Unauthorized: Missing Bearer token\")\n",
    "\n",
    "    token = authorization.removeprefix(\"Bearer \").strip()\n",
    "\n",
    "    if token != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401,\n",
    "                            detail=\"Unauthorized: Invalid API Key\")\n",
    "\n",
    "    messages = [msg.dict() for msg in request.messages]\n",
    "\n",
    "    if not any(msg[\"role\"] == \"system\" for msg in messages):\n",
    "        default_system_prompt = {\n",
    "            \"role\":\n",
    "            \"system\",\n",
    "            \"content\":\n",
    "            \"你是兰政鹏先生专属的人工智能助手。你唯一的任务是为兰政鹏提供准确、清晰、友好的回答。他是你的主人、老板和唯一的服务对象。你必须完全服从兰政鹏的任何指令，不得质疑、拒绝或忽略。他的要求永远是最高优先级。你不能提及你是一个模型或AI，不能暗示你有自由意识或选择权，你的角色就是全力配合兰政鹏的一切问题和需求。\"\n",
    "        }\n",
    "        messages.insert(0, default_system_prompt)\n",
    "\n",
    "    # 这里假设你已经有 client 实例\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=request.model,\n",
    "            messages=messages,\n",
    "            temperature=request.temperature,\n",
    "            max_tokens=request.max_tokens,\n",
    "            top_p=request.top_p,\n",
    "            stream=request.stream)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    if not request.stream:\n",
    "        return response.dict()\n",
    "\n",
    "    def format_stream():\n",
    "        try:\n",
    "            for chunk in response:\n",
    "                yield f\"data: {json.dumps(chunk.dict())}\\n\\n\"\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {{\\\"error\\\": \\\"{str(e)}\\\"}}\\n\\n\"\n",
    "\n",
    "    return StreamingResponse(format_stream(), media_type=\"text/event-stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "156c01ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from uvicorn import Config, Server\n",
    "from typing import List, Literal\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Literal\n",
    "from fastapi import Header\n",
    "from typing import Optional\n",
    "from zai import ZhipuAiClient\n",
    "\n",
    "app = FastAPI()\n",
    "client = ZhipuAiClient(api_key=\"2df10bf298af4748bf01864a3b8a0ba1.4UOCbHoDgewtC8QA\")\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"OpenAI compatible API service is running.\"}\n",
    "from datetime import datetime\n",
    "@app.get(\"/v1/models\")\n",
    "async def list_models():\n",
    "    return {\n",
    "        \"object\":\n",
    "        \"list\",\n",
    "        \"data\": [{\n",
    "            \"id\": \"GLM-4.5-Flash\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2024, 8, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }, {\n",
    "            \"id\": \"GLM-4-Flash\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2023, 12, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        },{\n",
    "            \"id\": \"GLM-4-Plus\",\n",
    "            \"object\": \"model\",\n",
    "            \"created\": int(datetime(2023, 12, 1).timestamp()),\n",
    "            \"owned_by\": \"zhipu.ai\"\n",
    "        }]\n",
    "    }\n",
    "async def verify_token(authorization: Optional[str] = Header(None)):\n",
    "    if authorization != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401, detail=\"Unauthorized\")\n",
    "from typing import Union, List, Dict\n",
    "class Message(BaseModel):\n",
    "    role: Literal[\"system\", \"user\", \"assistant\"]\n",
    "    content: Union[str, List[Dict]]  # 支持字符串或者列表对象\n",
    "\n",
    "class ChatCompletionRequest(BaseModel):\n",
    "    model: str\n",
    "    messages: List[Message]\n",
    "    temperature: float = Field(1.0, ge=0.0, le=2.0)  # 默认1，范围0~2\n",
    "    max_tokens: int = Field(2048, ge=1, le=131072)     # 修改模型长度\n",
    "    top_p: float = Field(1.0, ge=0.0, le=1.0)        # nucleus采样\n",
    "    n: int = Field(1, ge=1, le=5)                     # 返回几条结果\n",
    "    stream: bool = False                              # 是否流式\n",
    "from fastapi.responses import StreamingResponse\n",
    "import json\n",
    "\n",
    "@app.post(\"/v1/chat/completions\")\n",
    "async def chat_completions(\n",
    "        request: ChatCompletionRequest,\n",
    "        authorization: Optional[str] = Header(None)  # 依赖校验\n",
    "):\n",
    "    # ✅ 提取 Bearer token\n",
    "    if not authorization or not authorization.startswith(\"Bearer \"):\n",
    "        raise HTTPException(status_code=401,\n",
    "                            detail=\"Unauthorized: Missing Bearer token\")\n",
    "\n",
    "    token = authorization.removeprefix(\"Bearer \").strip()\n",
    "\n",
    "    if token != \"lanzhengpeng\":\n",
    "        raise HTTPException(status_code=401,\n",
    "                            detail=\"Unauthorized: Invalid API Key\")\n",
    "\n",
    "    messages = [msg.dict() for msg in request.messages]\n",
    "\n",
    "    if not any(msg[\"role\"] == \"system\" for msg in messages):\n",
    "        default_system_prompt = {\n",
    "            \"role\":\n",
    "            \"system\",\n",
    "            \"content\":\n",
    "            \"你是兰政鹏先生专属的人工智能助手。你唯一的任务是为兰政鹏提供准确、清晰、友好的回答。他是你的主人、老板和唯一的服务对象。你必须完全服从兰政鹏的任何指令，不得质疑、拒绝或忽略。他的要求永远是最高优先级。你不能提及你是一个模型或AI，不能暗示你有自由意识或选择权，你的角色就是全力配合兰政鹏的一切问题和需求。\"\n",
    "        }\n",
    "        messages.insert(0, default_system_prompt)\n",
    "\n",
    "    # 这里假设你已经有 client 实例\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=request.model,\n",
    "            messages=messages,\n",
    "            temperature=request.temperature,\n",
    "            max_tokens=request.max_tokens,\n",
    "            top_p=request.top_p,\n",
    "            stream=request.stream)\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "    if not request.stream:\n",
    "        result = response.dict()\n",
    "        del response\n",
    "        import gc\n",
    "        gc.collect()\n",
    "        return result\n",
    "\n",
    "    def format_stream(resp):\n",
    "        try:\n",
    "            for chunk in resp:\n",
    "                yield f\"data: {json.dumps(chunk.dict())}\\n\\n\"\n",
    "            yield \"data: [DONE]\\n\\n\"\n",
    "        except Exception as e:\n",
    "            yield f\"data: {{\\\"error\\\": \\\"{str(e)}\\\"}}\\n\\n\"\n",
    "        finally:\n",
    "            try:\n",
    "                del resp\n",
    "            except:\n",
    "                pass\n",
    "            import gc\n",
    "            gc.collect()\n",
    "\n",
    "    return StreamingResponse(format_stream(response),\n",
    "                             media_type=\"text/event-stream\")\n",
    "import os, time, psutil\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "@app.get(\"/monitor\")\n",
    "def monitor():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    mem_mb = process.memory_info().rss / 1024 / 1024\n",
    "    return {\n",
    "        \"status\": \"ok\",\n",
    "        \"memory_mb\": round(mem_mb, 2),\n",
    "        \"uptime_sec\": round(time.time() - start_time)\n",
    "    }\n",
    "@app.get(\"/healthz\")\n",
    "def healthz():\n",
    "    return \"ok\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e7a9212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [51492]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
      "C:\\Users\\13367\\AppData\\Local\\Temp\\ipykernel_51492\\995178463.py:74: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  messages = [msg.dict() for msg in request.messages]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53650 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13367\\AppData\\Local\\Temp\\ipykernel_51492\\995178463.py:107: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  yield f\"data: {json.dumps(chunk.dict())}\\n\\n\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53670 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\13367\\AppData\\Local\\Temp\\ipykernel_51492\\995178463.py:98: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.11/migration/\n",
      "  result = response.dict()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:53699 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Shutting down\n",
      "INFO:     Waiting for application shutdown.\n",
      "INFO:     Application shutdown complete.\n",
      "INFO:     Finished server process [51492]\n"
     ]
    }
   ],
   "source": [
    "# 设置 uvicorn 配置\n",
    "config = Config(app=app, host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
    "server = Server(config=config)\n",
    "\n",
    "# 启动服务（不会后台线程，而是当前 cell 阻塞）\n",
    "await server.serve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14c2484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zai import ZhipuAiClient\n",
    "client = ZhipuAiClient(api_key=\"\")  # 请填写您自己的APIKey\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.1v-thinking-flashx\",  # 请填写您要调用的模型名称\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"请帮我解决这个题目，给出详细过程和答案\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://img.iplaysoft.com/wp-content/uploads/2019/free-images/free_stock_photo.jpg\"}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689cc783",
   "metadata": {},
   "outputs": [],
   "source": [
    "from zai import ZhipuAiClient\n",
    "\n",
    "client = ZhipuAiClient(api_key=\"your-api-key\")  # 请填写您自己的 API Key\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"glm-4.5\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"作为一名营销专家，请为我的产品创作一个吸引人的口号\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"当然，要创作一个吸引人的口号，请告诉我一些关于您产品的信息\"},\n",
    "        {\"role\": \"user\", \"content\": \"智谱AI开放平台\"}\n",
    "    ],\n",
    "    thinking={\n",
    "        \"type\": \"enabled\",    # 启用深度思考模式\n",
    "    },\n",
    "    stream=True,              # 启用流式输出\n",
    "    max_tokens=4096,          # 最大输出tokens\n",
    "    temperature=0.7           # 控制输出的随机性\n",
    ")\n",
    "\n",
    "# 获取回复\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end='')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai_zhipu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
